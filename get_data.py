import requests
from bs4 import BeautifulSoup
import time
import os
import re
import csv
from datetime import datetime

BASE_URL = "https://bonbanh.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# ==== ÄÆ°á»ng dáº«n CSV ====
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# Táº¡o tÃªn file CSV theo ngÃ y cháº¡y
today_str = datetime.now().strftime("%Y-%m-%d")
CSV_FILE = os.path.join(DATA_DIR, f"bonbanh_raw_{today_str}.csv")

# Táº¡o file CSV + header náº¿u chÆ°a tá»“n táº¡i
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Loáº¡i xe + NÄƒm SX", "TÃªn xe", "GiÃ¡ xe_raw", "NÆ¡i bÃ¡n", "LiÃªn há»‡", "Link xe",
            "NgÃ y Ä‘Äƒng", "LÆ°á»£t xem", "Sá»‘ Km Ä‘Ã£ Ä‘i:", "TÃ¬nh tráº¡ng:", "Xuáº¥t xá»©:", "Kiá»ƒu dÃ¡ng:",
            "Äá»™ng cÆ¡:", "MÃ u ngoáº¡i tháº¥t:", "MÃ u ná»™i tháº¥t:", "Sá»‘ chá»— ngá»“i:", "Sá»‘ cá»­a:", "NÄƒm sáº£n xuáº¥t:"
        ])

# ====================================================================
# HÃ m ghi 1 dÃ²ng dá»¯ liá»‡u vÃ o file CSV
# ====================================================================
def append_csv(row_dict):
    """
    Ghi má»™t dÃ²ng thÃ´ng tin xe vÃ o file CSV.
    row_dict: dict chá»©a cÃ¡c trÆ°á»ng thÃ´ng tin cá»§a xe.
    """
    with open(CSV_FILE, "a", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            row_dict.get("Loáº¡i xe + NÄƒm SX", ""),
            row_dict.get("TÃªn xe", ""),
            row_dict.get("GiÃ¡ xe_raw", ""),
            row_dict.get("NÆ¡i bÃ¡n", ""),
            row_dict.get("LiÃªn há»‡", ""),
            row_dict.get("Link xe", ""),
            row_dict.get("NgÃ y Ä‘Äƒng", ""),
            row_dict.get("LÆ°á»£t xem", ""),
            row_dict.get("Sá»‘ Km Ä‘Ã£ Ä‘i:", ""),
            row_dict.get("TÃ¬nh tráº¡ng:", ""),
            row_dict.get("Xuáº¥t xá»©:", ""),
            row_dict.get("Kiá»ƒu dÃ¡ng:", ""),
            row_dict.get("Äá»™ng cÆ¡:", ""),
            row_dict.get("MÃ u ngoáº¡i tháº¥t:", ""),
            row_dict.get("MÃ u ná»™i tháº¥t:", ""),
            row_dict.get("Sá»‘ chá»— ngá»“i:", ""),
            row_dict.get("Sá»‘ cá»­a:", ""),
            row_dict.get("NÄƒm sáº£n xuáº¥t:", "")
        ])

# ====================================================================
# HÃ m táº£i HTML tá»« 1 URL
# ====================================================================
def get_page(url):
    """
    Gá»­i request Ä‘áº¿n URL vÃ  tráº£ vá» HTML.
    CÃ³ timeout vÃ  raise_for_status Ä‘á»ƒ bÃ¡o lá»—i khi request lá»—i.
    """
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    return resp.text

# ====================================================================
# HÃ m parse trang danh sÃ¡ch xe
# ====================================================================
def parse_list_page(html):
    """
    Parse HTML cá»§a trang danh sÃ¡ch Ä‘á»ƒ láº¥y:
    - Loáº¡i xe + nÄƒm
    - TÃªn xe
    - GiÃ¡
    - NÆ¡i bÃ¡n
    - LiÃªn há»‡
    - Link chi tiáº¿t

    Tráº£ vá» danh sÃ¡ch dict.
    """
    soup = BeautifulSoup(html, "html.parser")
    cars = []

    # .car-item = má»—i khá»‘i xe trÃªn trang bonbanh
    for item in soup.select(".car-item"):
        try:
            # Láº¥y link xe
            a_tag = item.select_one("a")
            link = ""
            if a_tag and a_tag.get("href"):
                href = a_tag["href"].strip()
                if not href.startswith("/"):
                    href = "/" + href
                link = BASE_URL + href

            # Loáº¡i xe vÃ  nÄƒm sáº£n xuáº¥t
            cb1 = item.select_one(".cb1")
            loai_xe = cb1.contents[0].strip() if cb1 and cb1.contents else ""
            nam_sx = cb1.select_one("b").get_text(strip=True) if cb1 and cb1.select_one("b") else ""
            info = f"{loai_xe} - {nam_sx}".strip(" -")

            # CÃ¡c trÆ°á»ng cÆ¡ báº£n
            ten_xe = item.select_one(".cb2 b").get_text(strip=True) if item.select_one(".cb2 b") else ""
            gia = item.select_one(".cb3 b").get_text(strip=True) if item.select_one(".cb3 b") else ""
            noi_ban = item.select_one(".cb4 b").get_text(strip=True) if item.select_one(".cb4 b") else ""
            lien_he = item.select_one(".cb7").get_text(" ", strip=True) if item.select_one(".cb7") else ""

            # LÆ°u vÃ o list
            cars.append({
                "Loáº¡i xe + NÄƒm SX": info,
                "TÃªn xe": ten_xe,
                "GiÃ¡ xe_raw": gia,
                "NÆ¡i bÃ¡n": noi_ban,
                "LiÃªn há»‡": lien_he,
                "Link xe": link,
            })

        except Exception as e:
            print(f"Lá»—i parse list item: {e}")
            continue

    return cars

# ====================================================================
# HÃ m parse trang chi tiáº¿t 1 xe
# ====================================================================
def parse_detail_page(url):
    """
    Parse HTML cá»§a trang chi tiáº¿t:
    - Láº¥y ngÃ y Ä‘Äƒng
    - LÆ°á»£t xem
    - Láº¥y cÃ¡c trÆ°á»ng chi tiáº¿t trong báº£ng thÃ´ng sá»‘ (div#mail_parent.row)
    """
    try:
        html = get_page(url)
        soup = BeautifulSoup(html, "html.parser")

        # ---- Láº¥y ngÃ y Ä‘Äƒng + lÆ°á»£t xem ----
        notes = soup.find("div", class_="notes")
        notes_text = notes.get_text(strip=True) if notes else ""

        ngay_dang = ""
        luot_xem = ""

        if notes_text:
            # Regex láº¥y ngÃ y Ä‘Äƒng
            m1 = re.search(r"ÄÄƒng\s+ngÃ y\s+(\d{1,2}/\d{1,2}/\d{4})", notes_text)
            if m1:
                ngay_dang = m1.group(1)

            # Regex láº¥y lÆ°á»£t xem
            m2 = re.search(r"Xem\s+(\d+)\s+lÆ°á»£t", notes_text)
            if m2:
                luot_xem = m2.group(1)

        # ---- Láº¥y thÃ´ng sá»‘ xe ----
        details = {}
        for row in soup.select("div#mail_parent.row"):
            label = row.find("label")
            value = row.find("span", class_="inp")
            if label and value:
                # VÃ­ dá»¥: "Sá»‘ cá»­a:" : "4"
                details[label.get_text(strip=True)] = value.get_text(strip=True)

        # Gom thÃ´ng tin láº¡i
        data = {"NgÃ y Ä‘Äƒng": ngay_dang, "LÆ°á»£t xem": luot_xem}
        data.update(details)

        return data

    except Exception as e:
        print(f"Lá»—i khi láº¥y chi tiáº¿t {url}: {e}")
        return {}

# ====================================================================
# HÃ m chÃ­nh cháº¡y crawl
# ====================================================================
def main():
    """
    Cháº¡y vÃ²ng láº·p qua cÃ¡c trang:
    - Crawl danh sÃ¡ch xe
    - Vá»›i má»—i xe: crawl thÃªm trang chi tiáº¿t
    - Ghi vÃ o CSV
    """
    all_count = 0

    # Duyá»‡t 1 trang (báº¡n cÃ³ thá»ƒ chá»‰nh range Ä‘á»ƒ crawl nhiá»u trang)
    for page in range(1, 2):
        url = f"{BASE_URL}/oto/page,{page}/" if page > 1 else BASE_URL
        print(f"Äang táº£i trang danh sÃ¡ch {page}... {url}")

        # Load HTML + parse danh sÃ¡ch
        html = get_page(url)
        car_list = parse_list_page(html)
        print(f"\nâ¡ TÃ¬m tháº¥y {len(car_list)} xe trÃªn trang {page}\n")

        # Duyá»‡t tá»«ng xe => láº¥y chi tiáº¿t
        for car in car_list:
            link = car.get("Link xe")
            if link:
                print(f"â†’ Láº¥y chi tiáº¿t: {link}")
                detail_data = parse_detail_page(link)

                # Gá»™p thÃ´ng tin list + detail
                car.update(detail_data)

                # Ghi CSV
                append_csv(car)

                all_count += 1
                time.sleep(1)  # trÃ¡nh bá»‹ cháº·n IP

    print(f"\nğŸ‰ ÄÃ£ crawl + ghi CSV {all_count} báº£n ghi.")

# ====================================================================
# Cháº¡y script
# ====================================================================
if __name__ == "__main__":
    main()
