import requests
from bs4 import BeautifulSoup
import time
import os
import re
import csv
from datetime import datetime
import logging

# ===========================
# Cáº¥u hÃ¬nh logger
# ===========================
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)
today_str_log = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
LOG_FILE = os.path.join(LOG_DIR, f"get_data_{today_str_log}.log")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("GetDataLogger")

# ===========================
# Cáº¥u hÃ¬nh crawl
# ===========================
BASE_URL = "https://bonbanh.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
today_str = datetime.now().strftime("%Y-%m-%d")
CSV_FILE = os.path.join(DATA_DIR, f"bonbanh_raw_{today_str}.csv")

if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow([
            "Loáº¡i xe + NÄƒm SX", "TÃªn xe", "GiÃ¡ xe_raw", "NÆ¡i bÃ¡n", "LiÃªn há»‡", "Link xe",
            "NgÃ y Ä‘Äƒng", "LÆ°á»£t xem", "Sá»‘ Km Ä‘Ã£ Ä‘i:", "TÃ¬nh tráº¡ng:", "Xuáº¥t xá»©:", "Kiá»ƒu dÃ¡ng:",
            "Äá»™ng cÆ¡:", "MÃ u ngoáº¡i tháº¥t:", "MÃ u ná»™i tháº¥t:", "Sá»‘ chá»— ngá»“i:", "Sá»‘ cá»­a:", "NÄƒm sáº£n xuáº¥t:"
        ])

# ===========================
# Ghi CSV
# ===========================
def append_csv(row_dict):
    try:
        with open(CSV_FILE, "a", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                row_dict.get("Loáº¡i xe + NÄƒm SX", ""),
                row_dict.get("TÃªn xe", ""),
                row_dict.get("GiÃ¡ xe_raw", ""),
                row_dict.get("NÆ¡i bÃ¡n", ""),
                row_dict.get("LiÃªn há»‡", ""),
                row_dict.get("Link xe", ""),
                row_dict.get("NgÃ y Ä‘Äƒng", ""),
                row_dict.get("LÆ°á»£t xem", ""),
                row_dict.get("Sá»‘ Km Ä‘Ã£ Ä‘i:", ""),
                row_dict.get("TÃ¬nh tráº¡ng:", ""),
                row_dict.get("Xuáº¥t xá»©:", ""),
                row_dict.get("Kiá»ƒu dÃ¡ng:", ""),
                row_dict.get("Äá»™ng cÆ¡:", ""),
                row_dict.get("MÃ u ngoáº¡i tháº¥t:", ""),
                row_dict.get("MÃ u ná»™i tháº¥t:", ""),
                row_dict.get("Sá»‘ chá»— ngá»“i:", ""),
                row_dict.get("Sá»‘ cá»­a:", ""),
                row_dict.get("NÄƒm sáº£n xuáº¥t:", "")
            ])
    except Exception as e:
        logger.exception("Lá»—i khi ghi CSV: %s", e)

# ===========================
# Láº¥y HTML tá»« URL
# ===========================
def get_page(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        logger.exception("Lá»—i khi táº£i URL %s: %s", url, e)
        return ""

# ===========================
# Parse trang danh sÃ¡ch
# ===========================
def parse_list_page(html):
    soup = BeautifulSoup(html, "html.parser")
    cars = []

    for item in soup.select(".car-item"):
        try:
            a_tag = item.select_one("a")
            link = ""
            if a_tag and a_tag.get("href"):
                href = a_tag["href"].strip()
                if not href.startswith("/"):
                    href = "/" + href
                link = BASE_URL + href

            cb1 = item.select_one(".cb1")
            loai_xe = cb1.contents[0].strip() if cb1 and cb1.contents else ""
            nam_sx = cb1.select_one("b").get_text(strip=True) if cb1 and cb1.select_one("b") else ""
            info = f"{loai_xe} - {nam_sx}".strip(" -")

            ten_xe = item.select_one(".cb2 b").get_text(strip=True) if item.select_one(".cb2 b") else ""
            gia = item.select_one(".cb3 b").get_text(strip=True) if item.select_one(".cb3 b") else ""
            noi_ban = item.select_one(".cb4 b").get_text(strip=True) if item.select_one(".cb4 b") else ""
            lien_he = item.select_one(".cb7").get_text(" ", strip=True) if item.select_one(".cb7") else ""

            cars.append({
                "Loáº¡i xe + NÄƒm SX": info,
                "TÃªn xe": ten_xe,
                "GiÃ¡ xe_raw": gia,
                "NÆ¡i bÃ¡n": noi_ban,
                "LiÃªn há»‡": lien_he,
                "Link xe": link,
            })

        except Exception as e:
            logger.exception("Lá»—i parse list item: %s", e)
            continue

    return cars

# ===========================
# Parse trang chi tiáº¿t
# ===========================
def parse_detail_page(url):
    try:
        html = get_page(url)
        soup = BeautifulSoup(html, "html.parser")

        notes = soup.find("div", class_="notes")
        notes_text = notes.get_text(strip=True) if notes else ""

        ngay_dang = ""
        luot_xem = ""

        if notes_text:
            m1 = re.search(r"ÄÄƒng\s+ngÃ y\s+(\d{1,2}/\d{1,2}/\d{4})", notes_text)
            if m1:
                ngay_dang = m1.group(1)
            m2 = re.search(r"Xem\s+(\d+)\s+lÆ°á»£t", notes_text)
            if m2:
                luot_xem = m2.group(1)

        details = {}
        for row in soup.select("div#mail_parent.row"):
            label = row.find("label")
            value = row.find("span", class_="inp")
            if label and value:
                details[label.get_text(strip=True)] = value.get_text(strip=True)

        data = {"NgÃ y Ä‘Äƒng": ngay_dang, "LÆ°á»£t xem": luot_xem}
        data.update(details)

        return data

    except Exception as e:
        logger.exception("Lá»—i khi láº¥y chi tiáº¿t %s: %s", url, e)
        return {}

# ===========================
# HÃ m main
# ===========================
def main():
    all_count = 0

    for page in range(1, 2):
        url = f"{BASE_URL}/oto/page,{page}/" if page > 1 else BASE_URL
        logger.info("Äang táº£i trang danh sÃ¡ch %d... %s", page, url)

        html = get_page(url)
        car_list = parse_list_page(html)
        logger.info("â¡ TÃ¬m tháº¥y %d xe trÃªn trang %d", len(car_list), page)

        for car in car_list:
            link = car.get("Link xe")
            if link:
                logger.info("â†’ Láº¥y chi tiáº¿t: %s", link)
                detail_data = parse_detail_page(link)
                car.update(detail_data)
                append_csv(car)
                all_count += 1
                time.sleep(1)

    logger.info("ğŸ‰ ÄÃ£ crawl + ghi CSV %d báº£n ghi.", all_count)

# ===========================
# Cháº¡y script
# ===========================
if __name__ == "__main__":
    main()
